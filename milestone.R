library(tm)
library(RWeka)
library(ggplot2)
library(dplyr)

#load data
twitter <- readLines('en_US.twitter.txt',skipNul=TRUE,  encoding="UTF-8")
news <- readLines('en_US.news.txt', skipNul=TRUE,  encoding="UTF-8")
blogs <- readLines('en_US.blogs.txt', skipNul=TRUE,  encoding="UTF-8")

#sampling
set.seed(39)
sampleTwitter <- sample(twitter, 0.01*length(twitter))
sampleNews <- sample(news, 0.01*length(news))
sampleBlogs <- sample(blogs, 0.01*length(blogs))
sampleData <- c(sampleTwitter,sampleNews,sampleBlogs)

rm(twitter)
rm(sampleTwitter)
rm(news)
rm(sampleNews)
rm(blogs)
rm(sampleBlogs)

#create and clean corpus
docs <- Corpus(VectorSource(sampleData))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)

#Ngram tokenization
options(mc.cores=1)
#uniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
#uniGramDTMatrix <- DocumentTermMatrix(docs, control = list(tokenize = uniGramTokenizer))
#termFrequency <- sort(colSums(as.matrix(uniGramDTMatrix)), decreasing=TRUE)
#termFrequency <- data.frame(word=names(termFrequency), freq=termFrequency)
#g1 <- ggplot(termFrequency[1:25,], aes(x=reorder(word, -freq), y=freq)) + geom_bar(stat="Identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab("Unigram") + ylab("Frequency") + labs(title = "Top 25 Unigrams by Frequency")
#print(g1)

biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
biGramDTMatrix <- DocumentTermMatrix(docs, control = list(tokenize = biGramTokenizer))
biGramDTMatrixSparse <- removeSparseTerms(biGramDTMatrix, 0.99995)
termFrequency2 <- sort(colSums(as.matrix(biGramDTMatrixSparse)), decreasing=TRUE)
termFrequency2 <- data.frame(word=names(termFrequency2), freq=termFrequency2)
save(termFrequency2, file = "BigramData.RData")
g2 <- ggplot(termFrequency2[1:50, ], aes(x=reorder(word, -freq), y=freq)) + geom_bar(stat="Identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab("Bigram") + ylab("Frequency") + labs(title = "Bigrams by Frequency")
print(g2)
rm(biGramDTMatrix)
rm(biGramDTMatrixSparse)
rm(termFrequency2)

triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
triGramDTMatrix <- DocumentTermMatrix(docs, control = list(tokenize = triGramTokenizer))
triGramDTMatrixSparse <- removeSparseTerms(triGramDTMatrix, 0.99995)
termFrequency3 <- sort(colSums(as.matrix(triGramDTMatrixSparse)), decreasing=TRUE)
termFrequency3 <- data.frame(word=names(termFrequency3), freq=termFrequency3)
save(termFrequency3, file = "TrigramData.RData")
g3 <- ggplot(termFrequency3[1:50, ], aes(x=reorder(word, -freq), y=freq)) + geom_bar(stat="Identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab("Trigram") + ylab("Frequency") + labs(title = "Trigrams by Frequency")
print(g3)
rm(triGramDTMatrix)
rm(triGramDTMatrixSparse)
rm(termFrequency3)

quadGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
quadGramDTMatrix <- DocumentTermMatrix(docs, control = list(tokenize = quadGramTokenizer))
quadGramDTMatrixSparse <- removeSparseTerms(quadGramDTMatrix, 0.99995)
termFrequency4 <- sort(colSums(as.matrix(quadGramDTMatrixSparse)), decreasing=TRUE)
termFrequency4 <- data.frame(word=names(termFrequency4), freq=termFrequency4)
save(termFrequency4, file = "QuadgramData.RData")
g4 <- ggplot(termFrequency4[1:50, ], aes(x=reorder(word, -freq), y=freq)) + geom_bar(stat="Identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + xlab("Quadgram") + ylab("Frequency") + labs(title = "Quadgrams by Frequency")
print(g4)
rm(quadGramDTMatrix)
rm(quadGramDTMatrixSparse)
rm(termFrequency4)
